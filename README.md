# Reading List for Machine Learning Group @ Tufts



## [Content](#content)

<table>

<tr><td colspan="2"><a href="#Graph-Neural-Networks">1. Graph Neural Networks</a></td></tr>
<tr>
    <td>&emsp;<a href="#basic-models">1.1 Basic Models</a></td>
    <td>&ensp;<a href="#graph-types">1.2 Graph Types</a></td>
</tr>
<tr>
    <td>&emsp;<a href="#pooling-methods">1.3 Pooling Methods</a></td>
    <td>&ensp;<a href="#analysis">1.4 Analysis</a></td>
</tr>

<tr><td colspan="2"><a href="#NLP">2. NLP</a></td></tr> 
<tr>
    <td>&emsp;<a href="#BERT-and-alike">2.1 BERT and alike</a></td>
    <td>&ensp;<a href="#Information-Extraction">2.2 Information Extraction </td>
</tr> 
<tr>
    <td>&emsp;<a href="#Parsing-trees">2.3 Parsing trees</td>
    <td>&ensp;<a href="#N4">2.4 N/A</td>
</tr>
<tr><td colspan="2"><a href="#Neural-Architecture-Search">3. Neural Architecture Search</a></td></tr> 
<tr>
    <td>&emsp;<a href="#Comparisons">3.1 Comparisions </a></td>
</tr> 
<tr><td colspan="2"><a href="#Probabilistic-Models">4. Probabilistic Models</a></td></tr> 
<tr>
    <td>&emsp;<a href="#Optimization">4.1 Theory </a></td>
</tr> 
</table>

## [Graph Neural Networks](#content)
### [Basic Models](#content)
1. **Benchmarking Graph Neural Networks.** arxiv 2020. [paper](https://arxiv.org/pdf/2003.00982.pdf)

*Vijay Prakash Dwivedi, Chaitanya K. Joshi , Thomas Laurent , Yoshua Bengio and Xavier Bresson.* 

2. **Graph Agreement Models for Semi-Supervised Learning.** NIPS 2019. [paper](https://papers.nips.cc/paper/9076-graph-agreement-models-for-semi-supervised-learning.pdf)

*Otilia Stretcu‡∗, Krishnamurthy Viswanathan†, Dana Movshovitz-Attias†, Emmanouil Antonios Platanios‡, Andrew Tomkins†, Sujith Ravi†*

3. **Position-aware Graph Neural Networks.** ICML 2019. [paper](https://arxiv.org/pdf/1906.04817.pdf)

*Jiaxuan You, Rex Ying, Jure Leskovec*

4. **A Flexible Generative Framework for Graph-based Semi-supervised Learning**  Neurips 2019. [paper](http://papers.nips.cc/paper/8590-a-flexible-generative-framework-for-graph-based-semi-supervised-learning.pdf)

*Jiaqi Ma, Weijing Tang, Ji Zhu, Qiaozhu Mei*

5. **Graph Transformer Networks** Neurips 2019. [paper](https://papers.nips.cc/paper/9367-graph-transformer-networks.pdf)

*Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, Hyunwoo J. Kim*


### [Graph Types](#content)

1. **Graph Convolutional Reinforcement Learning** ICLR 2020. [paper](https://openreview.net/pdf?id=HkxdQkSYDB)

*Jiechuan Jiang, Chen Dun, Tiejun Huang, Zongqing Lu*

### [Analysis](#content)
1. **Graph Neural Networks Exponentially Lose Expressive Power for Node Classification** ICLR 2020. [paper](https://openreview.net/pdf?id=S1ldO2EFPr)

*Kenta Oono, Taiji Suzuki*


## [NLP](#content)
### [BERT and alike](#content)
1. **Xlnet: Generalized autoregressive pretraining for language understanding.** NeurIPS 2019. [paper](https://arxiv.org/pdf/1906.08237.pdf)

*Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V.*

2. **Visualizing and Measuring the Geometry of BERT** NeurIPS 2019. [paper](https://papers.nips.cc/paper/9065-visualizing-and-measuring-the-geometry-of-bert.pdf)

*Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, Martin Wattenberg*

3. **Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context.** ACL 2019. [paper](https://www.aclweb.org/anthology/P19-1285.pdf)

*Dai, Zihang, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, and Ruslan Salakhutdinov.*

4. **BERT and PALs: Projected Attention Layers for Efficient Adaptation in Multi-Task Learning** ICML 2019. [paper](https://arxiv.org/abs/1902.02671)

*Asa Cooper Stickland, Iain Murray.*

5. **BERT Rediscovers the Classical NLP Pipeline** ACL 2019. [paper](https://arxiv.org/pdf/1905.05950.pdf)

*Tenney, I., Das, D. and Pavlick, E*

6. **What does BERT learn about the structure of language?** ACL 2019. [paper](https://www.aclweb.org/anthology/P19-1356.pdf)

*Jawahar, G., Sagot, B. and Seddah, D*

7. **Distilling Knowledge Learned in BERT for Text Generation** ACL 2020. [paper](https://arxiv.org/pdf/1911.03829.pdf)
*Yen-Chun Chen, Zhe Gan, Yu Cheng, Jingzhou Liu, Jingjing Liu*

8. **MASS: Masked Sequence to Sequence Pre-training for Language Generation** ICML 2019. [paper](https://arxiv.org/pdf/1905.02450.pdf)

*Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu*

### [Information Extraction](#content)
1. **Open information extraction with meta-pattern discovery in biomedical literature.** BCB2018 [paper](http://hanj.cs.illinois.edu/pdf/bcb18_xwang.pdf) 

*Wang, Xuan, Yu Zhang, Qi Li, Yinyin Chen, and Jiawei Han.*

2. **Cognitive Graph for Multi-Hop Reading Comprehension at Scale.** ACL 2019. [paper](https://arxiv.org/pdf/1905.05460.pdf)

*Ming Ding†, Chang Zhou‡, Qibin Chen†, Hongxia Yang‡, Jie Tang†*

3. **On the Possibilities and Limitations of Multi-hop Reasoning Under Linguistic Imperfections.** [paper](https://arxiv.org/pdf/1901.02522.pdf)

*Daniel Khashabi1∗ Erfan Sadeqi Azer2 Tushar Khot1 Ashish Sabharwal1 Dan Roth3*

4. **Semantic Graphs for Generating Deep Questions.** ACL 2020. [paper](https://arxiv.org/pdf/2004.12704.pdf)

*Liangming Pan, Yuxi Xie Yansong Feng Tat-Seng Chua Min-Yen Kan*

### [Parsing trees](#content)

1. **Unsupervised Recurrent Neural Network Grammars.** NAACL-HLT 2019. [paper](https://www.aclweb.org/anthology/N19-1114.pdf)

*Kim, Yoon, Alexander M. Rush, Lei Yu, Adhiguna Kuncoro, Chris Dyer, and Gábor Melis.*

## [Neural Architecture Search](#content)
### [Comparisons](#content)
1. **NAS evaluation is frustratingly hard.** ICLR 2020. [paper](https://openreview.net/pdf?id=HygrdpVKvr)

 *Yang, Antoine, Pedro M. Esperança, and Fabio M. Carlucci.*
 
## [Probabilistic Models](#content)
### [Optimization](#content)
1. **Computational Separations between Sampling and Optimization** Neurips 2019. [paper](https://papers.nips.cc/paper/9639-computational-separations-between-sampling-and-optimization.pdf)

*Kunal Talwar*

2. **Bayesian Optimization of Combinatorial Structures** ICML2018. [paper](https://arxiv.org/pdf/1806.08838.pdf)

*Ricardo Baptista, Matthias Poloczek*

